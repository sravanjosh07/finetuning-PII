{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training Data by Label\n",
    "\n",
    "This notebook extracts examples from existing datasets and organizes them by our 23 canonical labels.\n",
    "\n",
    "**Goal**: Create 23 JSON files, one per label, each containing diverse examples with subtypes.\n",
    "\n",
    "**Datasets used** (commercially safe):\n",
    "- nvidia/Nemotron-PII (100k samples) - CC BY 4.0\n",
    "- gretel-pii-masking-en-v1 (5k samples) - Apache 2.0\n",
    "- gretel-finance-multilingual - Apache 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = \"/Users/sravan/Documents/Experiments/fintuning_PII/Data\"\n",
    "OUTPUT_DIR = \"/Users/sravan/Documents/Experiments/fintuning_PII/Training_data\"\n",
    "\n",
    "# Create output directory if not exists\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. EXPLORE: Labels in Each Dataset\n",
    "\n",
    "**RUN THIS SECTION FIRST** to see all unique labels in each dataset before creating mappings.\n",
    "\n",
    "This helps you:\n",
    "1. See what labels exist in each dataset\n",
    "2. Find labels that need mapping\n",
    "3. Update the LABEL_MAPPING dictionary accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "NVIDIA NEMOTRON-PII LABELS\n",
      "======================================================================\n",
      "\n",
      "Total unique labels: 55\n",
      "Total samples: 100000\n",
      "\n",
      "Label                                Count    Examples\n",
      "----------------------------------------------------------------------\n",
      "first_name                           84043    Brian | Sabrina\n",
      "date                                 73867    20300615 | 20300615\n",
      "last_name                            59596    King | Garcia\n",
      "company_name                         54837    TransactFlow | Mercy Health Systems\n",
      "email                                53930    garciah@outlook.com | ismaeljgiacchetto@\n",
      "url                                  37847    https://secure.bankofamerica.com/legal-d\n",
      "occupation                           36888    secondary school teacher | Access Contro\n",
      "time                                 24506    00:33 | 15h45\n",
      "phone_number                         23930    931-613-1082 | 805-427-4731\n",
      "country                              23475    USA | USA\n",
      "customer_id                          20502    CUS498372 | 4873259160\n",
      "city                                 18347    Zanesville | Odessa\n",
      "state                                18318    OH | TN\n",
      "date_of_birth                        18079    1963-08-08 | 1983-09-16\n",
      "street_address                       16879    146 County Rd 86 | 122 S West Shore Blvd\n",
      "account_number                       16693    87425693 | 093-284359714\n",
      "user_name                            15800    lkernan | Janice2001\n",
      "date_time                            13221    2023-10-01T10:00:00Z | 2023-10-01T10:30:\n",
      "credit_debit_card                    12867    5490 3479 1287 6543 | 3742 912345 67890\n",
      "biometric_identifier                 11379    BIO-5729846130 | A48592361765\n",
      "medical_record_number                11098    M-24-000543 | M-24-000543\n",
      "employment_status                    11018    resigned | full-time\n",
      "health_plan_beneficiary_number       10558    TXMCD34827650 | 3FJ8-KP2-LR95\n",
      "education_level                       9378    high school | bachelor's degree\n",
      "race_ethnicity                        9248    Black | black\n",
      "employee_id                           8878    b4e7a2f1 | b4e7a2f1\n",
      "language                              8651    English | English\n",
      "county                                8465    Knox County | Clermont County\n",
      "bank_routing_number                   8354    271210785 | 123456789\n",
      "coordinate                            7659    40.7128 N, 74.0060 W | 40.7441 N, 73.984\n",
      "gender                                7645    Male | female\n",
      "age                                   7167    57 | 57\n",
      "password                              6985    Temporary Password | Temporary Password\n",
      "pin                                   6456    9283 | 542390\n",
      "fax_number                            6423    305-867-4980 | 747-606-0948\n",
      "political_view                        6402    Republican | Liberal\n",
      "postcode                              6279    79763 | 84045\n",
      "religious_belief                      6257    atheist | Christian\n",
      "ipv4                                  6078    186.210.18.154 | 14.135.72.239\n",
      "ssn                                   6062    274-39-9899 | 570-30-7083\n",
      "swift_bic                             5559    BKLMUS7RGT5 | XVZQUS4G98C\n",
      "blood_type                            5539    AB positive | A positive\n",
      "http_cookie                           4892    feature_test_group=variant_x7kz | gdpr_c\n",
      "cvv                                   4843    858 | 641\n",
      "api_key                               4667    read_dev_7BzVj9WpRmTkX2MfQdH9LgR5 | writ\n",
      "vehicle_identifier                    4561    3GNDF45265012108 | MCA1J46K24F921378\n",
      "mac_address                           4250    00:33:DE:1A:4F:7B | 00:39:F4:1E:5A:7B\n",
      "license_plate                         4078    5XKP298 | FTH-2854\n",
      "sexuality                             3312    queer | Straight\n",
      "ipv6                                  3139    2001:db8:85a3:0:0:8a2e:370:7334 | f3a1:7\n",
      "certificate_license_number            3002    98264157 | ENG-TX-20231045\n",
      "national_id                           2847    12345678901 | QX 23 98 15 5\n",
      "device_identifier                     2507    3b8a87d1-11e3-4a4d-82e4-72c3e81a1a1f | e\n",
      "unique_id                             1777    9b2a3f84-84d3-4b0b-9c2b-5a5f899f818e | 9\n",
      "tax_id                                1302    43-5478236 | 937-48-2658\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# EXPLORE NVIDIA NEMOTRON-PII LABELS\n",
    "# ============================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"NVIDIA NEMOTRON-PII LABELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "nvidia_path = f\"{DATA_DIR}/nvidia-nemotron-pii/test.json\"\n",
    "with open(nvidia_path, 'r') as f:\n",
    "    nvidia_data = json.load(f)\n",
    "\n",
    "nvidia_labels = Counter()\n",
    "nvidia_examples = defaultdict(list)  # Store example for each label\n",
    "\n",
    "for item in nvidia_data:\n",
    "    text = item.get('text', '')\n",
    "    spans = item.get('spans', [])\n",
    "    if isinstance(spans, str):\n",
    "        try:\n",
    "            spans = ast.literal_eval(spans)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for span in spans:\n",
    "        if isinstance(span, dict):\n",
    "            label = span.get('label', '')\n",
    "            nvidia_labels[label] += 1\n",
    "            # Store one example per label\n",
    "            if len(nvidia_examples[label]) < 2:\n",
    "                start, end = span.get('start', 0), span.get('end', 0)\n",
    "                entity_text = text[start:end] if start < end else ''\n",
    "                nvidia_examples[label].append(entity_text[:50])\n",
    "\n",
    "print(f\"\\nTotal unique labels: {len(nvidia_labels)}\")\n",
    "print(f\"Total samples: {len(nvidia_data)}\")\n",
    "print(\"\\nLabel                                Count    Examples\")\n",
    "print(\"-\" * 70)\n",
    "for label, count in nvidia_labels.most_common():\n",
    "    examples = nvidia_examples[label][:2]\n",
    "    ex_str = ' | '.join(examples)[:40]\n",
    "    print(f\"{label:35s} {count:6d}    {ex_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GRETEL PII-MASKING-EN-V1 LABELS\n",
      "======================================================================\n",
      "\n",
      "Total unique labels: 42\n",
      "Total samples: 5000\n",
      "\n",
      "Label                                Count    Examples\n",
      "----------------------------------------------------------------------\n",
      "medical_record_number                 2658    MRN-293104 | MED25315002\n",
      "date_of_birth                         2331    1960-11-14 | 1975-04-21\n",
      "ssn                                   1661    433-42-5929 | ZZ736903T\n",
      "first_name                            1172    Ekanta | Louise\n",
      "date                                  1157    1989.12.22 | 1997-01-06\n",
      "last_name                             1057    Purohit | Tripathi\n",
      "email                                 1049    veronicawood@example.org | xwilliams@exa\n",
      "customer_id                           1033    CID-996335 | D-870175-E\n",
      "employee_id                           1005    EMP730359 | B5890579\n",
      "name                                   980    Zaitra Sarma | Heather Johnson\n",
      "phone_number                           904    +1-869-341-9301x7005 | 280.900.0632x9032\n",
      "ipv4                                   896    216.158.70.128 | 136.129.142.198\n",
      "street_address                         869    07570 Joanna Mountains, 67789, South Bar\n",
      "credit_card_number                     663    3548 6974 7128 1306 | 3530-2910-7196-\n",
      "license_plate                          579    KS40540825 | P84920793\n",
      "address                                563    Suite 378, Yolanda Mountain, Burkeberg |\n",
      "user_name                              338    tw_brian740 | rhys95\n",
      "bank_routing_number                    257    051816242 | 360739620\n",
      "device_identifier                      249    213226001122238 | 356929026872567\n",
      "date_time                              211    12/16/2008 09:17 AM | 1993-01-10 19:46:0\n",
      "unique_identifier                      189    540K-L53Q-NWDV | IDRPRS1ZCPP3\n",
      "company_name                           185    Chapman PLC | Smith-Taylor\n",
      "account_number                         141    E11248142150 | F62721280238\n",
      "biometric_identifier                   137    BIO-4010357692 | A19023156139\n",
      "city                                   128    Munger | Tiruvottiyur\n",
      "certificate_license_number             124    LIC-I9611592 | CERT-48865042\n",
      "time                                   106    12:45:33.226001 | 11:45 AM\n",
      "postcode                               104    LN0Y 4PT | 319955\n",
      "vehicle_identifier                      98    8HEC1TN717CH8S6J9 | 1804SFN3B25PDUDGY\n",
      "coordinate                              85    56.553299, -26.524318 | Latitude: 20.808\n",
      "country                                 71    South Georgia and the South Sandwich Isl\n",
      "ipv6                                    66    f4e3:117d:3857:870:d854:1ddb:ef90:66f1 |\n",
      "api_key                                 60    sl.SloV2VW-92QT0rRJY8yhbQG21cbs | xoxb-4\n",
      "password                                59    7X6OygsRVFi$E@vu+( | Os6l)zfdO_%27fKt^T\n",
      "national_id                             46    257871725 | 188488901\n",
      "health_plan_beneficiary_number          41    G934357132 | I788256308\n",
      "swift_bic                               28    KUBODEKA052 | XJDRGBKL960\n",
      "state                                   27    TAS | Andhra Pradesh\n",
      "tax_id                                  23    70418552308 | 79872830350\n",
      "url                                     23    http://www.jackson.com/ | http://wright.\n",
      "cvv                                      3    474 | 632\n",
      "pin                                      2    607137 | 2451\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# EXPLORE GRETEL PII-MASKING LABELS\n",
    "# ============================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"GRETEL PII-MASKING-EN-V1 LABELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "gretel_path = f\"{DATA_DIR}/gretel-pii-masking-en-v1/test.json\"\n",
    "with open(gretel_path, 'r') as f:\n",
    "    gretel_data = json.load(f)\n",
    "\n",
    "gretel_labels = Counter()\n",
    "gretel_examples = defaultdict(list)\n",
    "\n",
    "for item in gretel_data:\n",
    "    entities_raw = item.get('entities', '[]')\n",
    "    if isinstance(entities_raw, str):\n",
    "        try:\n",
    "            entities_raw = ast.literal_eval(entities_raw)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for ent in entities_raw:\n",
    "        entity_text = ent.get('entity', '')\n",
    "        types = ent.get('types', [])\n",
    "        for t in types:\n",
    "            gretel_labels[t] += 1\n",
    "            if len(gretel_examples[t]) < 2:\n",
    "                gretel_examples[t].append(entity_text[:50])\n",
    "\n",
    "print(f\"\\nTotal unique labels: {len(gretel_labels)}\")\n",
    "print(f\"Total samples: {len(gretel_data)}\")\n",
    "print(\"\\nLabel                                Count    Examples\")\n",
    "print(\"-\" * 70)\n",
    "for label, count in gretel_labels.most_common():\n",
    "    examples = gretel_examples[label][:2]\n",
    "    ex_str = ' | '.join(examples)[:40]\n",
    "    print(f\"{label:35s} {count:6d}    {ex_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "GRETEL FINANCE MULTILINGUAL LABELS\n",
      "======================================================================\n",
      "\n",
      "Total unique labels: 0\n",
      "Total samples: 5594\n",
      "\n",
      "Label                                Count    Examples\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# EXPLORE GRETEL FINANCE MULTILINGUAL LABELS\n",
    "# ============================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"GRETEL FINANCE MULTILINGUAL LABELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "gretel_fin_path = f\"{DATA_DIR}/gretel-finance-multilingual/test.json\"\n",
    "with open(gretel_fin_path, 'r') as f:\n",
    "    gretel_fin_data = json.load(f)\n",
    "\n",
    "gretel_fin_labels = Counter()\n",
    "gretel_fin_examples = defaultdict(list)\n",
    "\n",
    "for item in gretel_fin_data:\n",
    "    entities_raw = item.get('entities', '[]')\n",
    "    if isinstance(entities_raw, str):\n",
    "        try:\n",
    "            entities_raw = ast.literal_eval(entities_raw)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    for ent in entities_raw:\n",
    "        entity_text = ent.get('entity', '')\n",
    "        types = ent.get('types', [])\n",
    "        for t in types:\n",
    "            gretel_fin_labels[t] += 1\n",
    "            if len(gretel_fin_examples[t]) < 2:\n",
    "                gretel_fin_examples[t].append(entity_text[:50])\n",
    "\n",
    "print(f\"\\nTotal unique labels: {len(gretel_fin_labels)}\")\n",
    "print(f\"Total samples: {len(gretel_fin_data)}\")\n",
    "print(\"\\nLabel                                Count    Examples\")\n",
    "print(\"-\" * 70)\n",
    "for label, count in gretel_fin_labels.most_common():\n",
    "    examples = gretel_fin_examples[label][:2]\n",
    "    ex_str = ' | '.join(examples)[:40]\n",
    "    print(f\"{label:35s} {count:6d}    {ex_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "SUMMARY: ALL UNIQUE LABELS (for mapping)\n",
      "======================================================================\n",
      "\n",
      "Total unique labels across all datasets: 59\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Label                               NVIDIA   GRETEL   GRETEL-FIN\n",
      "----------------------------------------------------------------------\n",
      "account_number                       16693      141        0\n",
      "address                                  0      563        0\n",
      "age                                   7167        0        0\n",
      "api_key                               4667       60        0\n",
      "bank_routing_number                   8354      257        0\n",
      "biometric_identifier                 11379      137        0\n",
      "blood_type                            5539        0        0\n",
      "certificate_license_number            3002      124        0\n",
      "city                                 18347      128        0\n",
      "company_name                         54837      185        0\n",
      "coordinate                            7659       85        0\n",
      "country                              23475       71        0\n",
      "county                                8465        0        0\n",
      "credit_card_number                       0      663        0\n",
      "credit_debit_card                    12867        0        0\n",
      "customer_id                          20502     1033        0\n",
      "cvv                                   4843        3        0\n",
      "date                                 73867     1157        0\n",
      "date_of_birth                        18079     2331        0\n",
      "date_time                            13221      211        0\n",
      "device_identifier                     2507      249        0\n",
      "education_level                       9378        0        0\n",
      "email                                53930     1049        0\n",
      "employee_id                           8878     1005        0\n",
      "employment_status                    11018        0        0\n",
      "fax_number                            6423        0        0\n",
      "first_name                           84043     1172        0\n",
      "gender                                7645        0        0\n",
      "health_plan_beneficiary_number       10558       41        0\n",
      "http_cookie                           4892        0        0\n",
      "ipv4                                  6078      896        0\n",
      "ipv6                                  3139       66        0\n",
      "language                              8651        0        0\n",
      "last_name                            59596     1057        0\n",
      "license_plate                         4078      579        0\n",
      "mac_address                           4250        0        0\n",
      "medical_record_number                11098     2658        0\n",
      "name                                     0      980        0\n",
      "national_id                           2847       46        0\n",
      "occupation                           36888        0        0\n",
      "password                              6985       59        0\n",
      "phone_number                         23930      904        0\n",
      "pin                                   6456        2        0\n",
      "political_view                        6402        0        0\n",
      "postcode                              6279      104        0\n",
      "race_ethnicity                        9248        0        0\n",
      "religious_belief                      6257        0        0\n",
      "sexuality                             3312        0        0\n",
      "ssn                                   6062     1661        0\n",
      "state                                18318       27        0\n",
      "street_address                       16879      869        0\n",
      "swift_bic                             5559       28        0\n",
      "tax_id                                1302       23        0\n",
      "time                                 24506      106        0\n",
      "unique_id                             1777        0        0\n",
      "unique_identifier                        0      189        0\n",
      "url                                  37847       23        0\n",
      "user_name                            15800      338        0\n",
      "vehicle_identifier                    4561       98        0\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# SUMMARY: ALL UNIQUE LABELS ACROSS DATASETS\n",
    "# ============================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"SUMMARY: ALL UNIQUE LABELS (for mapping)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "all_source_labels = set(nvidia_labels.keys()) | set(gretel_labels.keys()) | set(gretel_fin_labels.keys())\n",
    "print(f\"\\nTotal unique labels across all datasets: {len(all_source_labels)}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 70)\n",
    "print(\"Label                               NVIDIA   GRETEL   GRETEL-FIN\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for label in sorted(all_source_labels):\n",
    "    nv = nvidia_labels.get(label, 0)\n",
    "    gr = gretel_labels.get(label, 0)\n",
    "    gf = gretel_fin_labels.get(label, 0)\n",
    "    print(f\"{label:35s} {nv:6d}   {gr:6d}   {gf:6d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Define Label Mapping\n",
    "\n",
    "**UPDATE THE MAPPING BELOW** based on the labels you discovered above.\n",
    "\n",
    "Map dataset-specific labels → (canonical_label, subtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping from source labels -> (canonical_label, subtype)\n",
    "LABEL_MAPPING = {\n",
    "    # === FULL NAME ===\n",
    "    \"first_name\": (\"full name\", \"person_name\"),\n",
    "    \"last_name\": (\"full name\", \"person_name\"),\n",
    "    \"name\": (\"full name\", \"person_name\"),\n",
    "    \"person\": (\"full name\", \"person_name\"),\n",
    "    \"patient_name\": (\"full name\", \"patient_name\"),\n",
    "    \"doctor_name\": (\"full name\", \"doctor_name\"),\n",
    "    \n",
    "    # === DATE ===\n",
    "    \"date\": (\"date\", \"general_date\"),\n",
    "    \"date_of_birth\": (\"date\", \"date_of_birth\"),\n",
    "    \"date_time\": (\"date\", \"datetime\"),\n",
    "    \"time\": (\"date\", \"time\"),\n",
    "    \"expiration_date\": (\"date\", \"expiration_date\"),\n",
    "    \n",
    "    # === ADDRESS ===\n",
    "    \"address\": (\"address\", \"general_address\"),\n",
    "    \"street_address\": (\"address\", \"street_address\"),\n",
    "    \"city\": (\"address\", \"city\"),\n",
    "    \"state\": (\"address\", \"state\"),\n",
    "    \"country\": (\"address\", \"country\"),\n",
    "    \"postcode\": (\"address\", \"postal_code\"),\n",
    "    \"coordinate\": (\"address\", \"coordinates\"),\n",
    "    \n",
    "    # === PHONE ===\n",
    "    \"phone_number\": (\"phone number\", \"general_phone\"),\n",
    "    \"mobile_phone_number\": (\"phone number\", \"mobile_phone\"),\n",
    "    \n",
    "    # === FAX ===\n",
    "    \"fax_number\": (\"fax number\", \"fax\"),\n",
    "    \n",
    "    # === EMAIL ===\n",
    "    \"email\": (\"email address\", \"email\"),\n",
    "    \"email_address\": (\"email address\", \"email\"),\n",
    "    \n",
    "    # === SSN ===\n",
    "    \"ssn\": (\"social security number\", \"ssn\"),\n",
    "    \"social_security_number\": (\"social security number\", \"ssn\"),\n",
    "    \n",
    "    # === CREDIT CARD ===\n",
    "    \"credit_card_number\": (\"credit card number\", \"credit_card\"),\n",
    "    \"credit_debit_card\": (\"credit card number\", \"credit_debit_card\"),\n",
    "    \n",
    "    # === BANK ACCOUNT ===\n",
    "    \"account_number\": (\"bank account number\", \"account_number\"),\n",
    "    \"bank_routing_number\": (\"bank account number\", \"routing_number\"),\n",
    "    \n",
    "    # === AMOUNT ===\n",
    "    \"amount\": (\"amount\", \"general_amount\"),\n",
    "    \"transaction_amount\": (\"amount\", \"transaction_amount\"),\n",
    "    \"salary\": (\"amount\", \"salary\"),\n",
    "    \n",
    "    # === CREDIT SCORE ===\n",
    "    \"credit_score\": (\"credit score\", \"credit_score\"),\n",
    "    \n",
    "    # === IBAN ===\n",
    "    \"iban\": (\"iban\", \"iban\"),\n",
    "    \"swift_bic\": (\"iban\", \"swift_code\"),\n",
    "    \n",
    "    # === TAX ID ===\n",
    "    \"tax_id\": (\"tax identification number\", \"tax_id\"),\n",
    "    \"tax_identification_number\": (\"tax identification number\", \"tax_id\"),\n",
    "    \n",
    "    # === DRIVER'S LICENSE ===\n",
    "    \"driver_license\": (\"driver's license number\", \"drivers_license\"),\n",
    "    \"drivers_license_number\": (\"driver's license number\", \"drivers_license\"),\n",
    "    \n",
    "    # === PASSPORT ===\n",
    "    \"passport_number\": (\"passport number\", \"passport\"),\n",
    "    \"passport\": (\"passport number\", \"passport\"),\n",
    "    \n",
    "    # === ID NUMBER ===\n",
    "    \"national_id\": (\"identification number\", \"national_id\"),\n",
    "    \"identity_card_number\": (\"identification number\", \"identity_card\"),\n",
    "    \"employee_id\": (\"identification number\", \"employee_id\"),\n",
    "    \"customer_id\": (\"identification number\", \"customer_id\"),\n",
    "    \"unique_identifier\": (\"identification number\", \"unique_id\"),\n",
    "    \"certificate_license_number\": (\"identification number\", \"certificate_number\"),\n",
    "    \"medical_record_number\": (\"identification number\", \"medical_record_number\"),\n",
    "    \n",
    "    # === INSURANCE ===\n",
    "    \"health_insurance_id_number\": (\"insurance number\", \"health_insurance\"),\n",
    "    \"health_plan_beneficiary_number\": (\"insurance number\", \"health_plan\"),\n",
    "    \"insurance_number\": (\"insurance number\", \"insurance\"),\n",
    "    \n",
    "    # === IP ADDRESS ===\n",
    "    \"ip_address\": (\"ip address\", \"ip\"),\n",
    "    \"ipv4\": (\"ip address\", \"ipv4\"),\n",
    "    \"ipv6\": (\"ip address\", \"ipv6\"),\n",
    "    \n",
    "    # === USERNAME ===\n",
    "    \"username\": (\"username\", \"username\"),\n",
    "    \"user_name\": (\"username\", \"username\"),\n",
    "    \"user_id\": (\"username\", \"user_id\"),\n",
    "    \n",
    "    # === ORGANIZATION ===\n",
    "    \"organization\": (\"organization\", \"organization\"),\n",
    "    \"company_name\": (\"organization\", \"company\"),\n",
    "    \"hospital\": (\"organization\", \"hospital\"),\n",
    "    \n",
    "    # === MEDICAL CONDITION ===\n",
    "    \"medical_condition\": (\"medical condition\", \"condition\"),\n",
    "    \"diagnosis\": (\"medical condition\", \"diagnosis\"),\n",
    "    \"disease\": (\"medical condition\", \"disease\"),\n",
    "    \n",
    "    # === MEDICAL TREATMENT ===\n",
    "    \"medical_treatment\": (\"medical treatment\", \"treatment\"),\n",
    "    \"procedure\": (\"medical treatment\", \"procedure\"),\n",
    "    \"therapy\": (\"medical treatment\", \"therapy\"),\n",
    "    \n",
    "    # === MEDICATION ===\n",
    "    \"medication\": (\"medication\", \"medication\"),\n",
    "    \"drug_name\": (\"medication\", \"drug\"),\n",
    "    \"prescription\": (\"medication\", \"prescription\"),\n",
    "}\n",
    "\n",
    "# Our 23 canonical labels\n",
    "CANONICAL_LABELS = [\n",
    "    \"full name\", \"date\", \"address\", \"phone number\", \"fax number\",\n",
    "    \"email address\", \"social security number\", \"credit card number\",\n",
    "    \"bank account number\", \"amount\", \"credit score\", \"iban\",\n",
    "    \"tax identification number\", \"driver's license number\", \"passport number\",\n",
    "    \"identification number\", \"insurance number\", \"ip address\", \"username\",\n",
    "    \"organization\", \"medical condition\", \"medical treatment\", \"medication\"\n",
    "]\n",
    "\n",
    "print(f\"Defined {len(LABEL_MAPPING)} source label mappings\")\n",
    "print(f\"Target: {len(CANONICAL_LABELS)} canonical labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CHECK: UNMAPPED LABELS\n",
    "# Run this after defining LABEL_MAPPING to find missing mappings\n",
    "# ============================================================\n",
    "print(\"=\" * 70)\n",
    "print(\"UNMAPPED LABELS (labels in datasets but NOT in LABEL_MAPPING)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "unmapped = []\n",
    "for label in all_source_labels:\n",
    "    # Normalize for comparison\n",
    "    normalized = label.lower().replace(' ', '_').replace('-', '_')\n",
    "    if normalized not in LABEL_MAPPING:\n",
    "        total = nvidia_labels.get(label, 0) + gretel_labels.get(label, 0) + gretel_fin_labels.get(label, 0)\n",
    "        unmapped.append((label, total))\n",
    "\n",
    "unmapped.sort(key=lambda x: -x[1])  # Sort by count descending\n",
    "\n",
    "if unmapped:\n",
    "    print(f\"\\nFound {len(unmapped)} unmapped labels:\")\n",
    "    print(\"\\nLabel                               Total Count   Action Needed\")\n",
    "    print(\"-\" * 70)\n",
    "    for label, count in unmapped:\n",
    "        print(f\"{label:35s} {count:6d}        # TODO: add mapping\")\n",
    "else:\n",
    "    print(\"\\n✓ All labels are mapped!\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"To add a mapping, use format:\")\n",
    "print('  \"source_label\": (\"canonical_label\", \"subtype\"),')\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nvidia_data():\n",
    "    \"\"\"Load nvidia/Nemotron-PII dataset\"\"\"\n",
    "    path = f\"{DATA_DIR}/nvidia-nemotron-pii/test.json\"\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    samples = []\n",
    "    for item in data:\n",
    "        text = item.get('text', '')\n",
    "        spans = item.get('spans', [])\n",
    "        \n",
    "        # Parse spans if string\n",
    "        if isinstance(spans, str):\n",
    "            try:\n",
    "                spans = ast.literal_eval(spans)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        entities = []\n",
    "        for span in spans:\n",
    "            if isinstance(span, dict):\n",
    "                label = span.get('label', '').lower().replace(' ', '_')\n",
    "                start = span.get('start', 0)\n",
    "                end = span.get('end', 0)\n",
    "                entity_text = text[start:end] if start < end else span.get('text', '')\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            if label in LABEL_MAPPING:\n",
    "                canonical, subtype = LABEL_MAPPING[label]\n",
    "                entities.append({\n",
    "                    'text': entity_text,\n",
    "                    'label': canonical,\n",
    "                    'subtype': subtype,\n",
    "                    'original_label': label,\n",
    "                    'start': start,\n",
    "                    'end': end\n",
    "                })\n",
    "        \n",
    "        if entities:\n",
    "            samples.append({\n",
    "                'text': text,\n",
    "                'entities': entities,\n",
    "                'source': 'nvidia-nemotron'\n",
    "            })\n",
    "    \n",
    "    print(f\"Loaded {len(samples)} samples from nvidia-nemotron\")\n",
    "    return samples\n",
    "\n",
    "nvidia_samples = load_nvidia_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gretel_data():\n",
    "    \"\"\"Load gretel-pii-masking dataset\"\"\"\n",
    "    path = f\"{DATA_DIR}/gretel-pii-masking-en-v1/test.json\"\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    samples = []\n",
    "    for item in data:\n",
    "        text = item.get('text', '')\n",
    "        entities_raw = item.get('entities', '[]')\n",
    "        \n",
    "        # Parse entities if string\n",
    "        if isinstance(entities_raw, str):\n",
    "            try:\n",
    "                entities_raw = ast.literal_eval(entities_raw)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        entities = []\n",
    "        for ent in entities_raw:\n",
    "            entity_text = ent.get('entity', '')\n",
    "            types = ent.get('types', [])\n",
    "            \n",
    "            for t in types:\n",
    "                label = t.lower().replace(' ', '_')\n",
    "                if label in LABEL_MAPPING:\n",
    "                    canonical, subtype = LABEL_MAPPING[label]\n",
    "                    \n",
    "                    # Find position in text\n",
    "                    start = text.find(entity_text)\n",
    "                    end = start + len(entity_text) if start >= 0 else 0\n",
    "                    \n",
    "                    entities.append({\n",
    "                        'text': entity_text,\n",
    "                        'label': canonical,\n",
    "                        'subtype': subtype,\n",
    "                        'original_label': label,\n",
    "                        'start': start,\n",
    "                        'end': end\n",
    "                    })\n",
    "        \n",
    "        if entities:\n",
    "            samples.append({\n",
    "                'text': text,\n",
    "                'entities': entities,\n",
    "                'source': 'gretel'\n",
    "            })\n",
    "    \n",
    "    print(f\"Loaded {len(samples)} samples from gretel\")\n",
    "    return samples\n",
    "\n",
    "gretel_samples = load_gretel_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gretel_finance():\n",
    "    \"\"\"Load gretel-finance-multilingual dataset\"\"\"\n",
    "    path = f\"{DATA_DIR}/gretel-finance-multilingual/test.json\"\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    samples = []\n",
    "    for item in data:\n",
    "        text = item.get('text', '')\n",
    "        entities_raw = item.get('entities', '[]')\n",
    "        \n",
    "        if isinstance(entities_raw, str):\n",
    "            try:\n",
    "                entities_raw = ast.literal_eval(entities_raw)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        entities = []\n",
    "        for ent in entities_raw:\n",
    "            entity_text = ent.get('entity', '')\n",
    "            types = ent.get('types', [])\n",
    "            \n",
    "            for t in types:\n",
    "                label = t.lower().replace(' ', '_')\n",
    "                if label in LABEL_MAPPING:\n",
    "                    canonical, subtype = LABEL_MAPPING[label]\n",
    "                    start = text.find(entity_text)\n",
    "                    end = start + len(entity_text) if start >= 0 else 0\n",
    "                    \n",
    "                    entities.append({\n",
    "                        'text': entity_text,\n",
    "                        'label': canonical,\n",
    "                        'subtype': subtype,\n",
    "                        'original_label': label,\n",
    "                        'start': start,\n",
    "                        'end': end\n",
    "                    })\n",
    "        \n",
    "        if entities:\n",
    "            samples.append({\n",
    "                'text': text,\n",
    "                'entities': entities,\n",
    "                'source': 'gretel-finance'\n",
    "            })\n",
    "    \n",
    "    print(f\"Loaded {len(samples)} samples from gretel-finance\")\n",
    "    return samples\n",
    "\n",
    "gretel_finance_samples = load_gretel_finance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all samples\n",
    "all_samples = nvidia_samples + gretel_samples + gretel_finance_samples\n",
    "print(f\"\\nTotal samples: {len(all_samples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Organize by Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group samples by canonical label\n",
    "samples_by_label = defaultdict(list)\n",
    "\n",
    "for sample in all_samples:\n",
    "    for entity in sample['entities']:\n",
    "        label = entity['label']\n",
    "        samples_by_label[label].append({\n",
    "            'text': sample['text'],\n",
    "            'entity': entity,\n",
    "            'source': sample['source']\n",
    "        })\n",
    "\n",
    "# Show distribution\n",
    "print(\"Samples per label:\")\n",
    "print(\"=\" * 50)\n",
    "for label in CANONICAL_LABELS:\n",
    "    count = len(samples_by_label[label])\n",
    "    print(f\"{count:6d}  {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show subtype distribution for each label\n",
    "print(\"\\nSubtype distribution per label:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for label in CANONICAL_LABELS:\n",
    "    subtypes = Counter()\n",
    "    for sample in samples_by_label[label]:\n",
    "        subtypes[sample['entity']['subtype']] += 1\n",
    "    \n",
    "    if subtypes:\n",
    "        print(f\"\\n{label}:\")\n",
    "        for subtype, count in subtypes.most_common():\n",
    "            print(f\"  {count:5d}  {subtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create JSON Files per Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_file(label, samples, max_per_subtype=200):\n",
    "    \"\"\"Create a JSON file for a specific label with diverse subtypes\"\"\"\n",
    "    \n",
    "    # Group by subtype\n",
    "    by_subtype = defaultdict(list)\n",
    "    for sample in samples:\n",
    "        subtype = sample['entity']['subtype']\n",
    "        by_subtype[subtype].append(sample)\n",
    "    \n",
    "    # Sample from each subtype (balanced)\n",
    "    final_samples = []\n",
    "    for subtype, subtype_samples in by_subtype.items():\n",
    "        # Shuffle and take up to max_per_subtype\n",
    "        random.shuffle(subtype_samples)\n",
    "        selected = subtype_samples[:max_per_subtype]\n",
    "        \n",
    "        for s in selected:\n",
    "            final_samples.append({\n",
    "                'text': s['text'],\n",
    "                'entities': [{\n",
    "                    'text': s['entity']['text'],\n",
    "                    'label': label,\n",
    "                    'subtype': subtype,\n",
    "                    'start': s['entity']['start'],\n",
    "                    'end': s['entity']['end']\n",
    "                }],\n",
    "                'source': s['source']\n",
    "            })\n",
    "    \n",
    "    return final_samples, dict(by_subtype)\n",
    "\n",
    "# Create files for each label\n",
    "random.seed(42)  # For reproducibility\n",
    "\n",
    "for label in CANONICAL_LABELS:\n",
    "    samples = samples_by_label[label]\n",
    "    \n",
    "    if not samples:\n",
    "        print(f\"WARNING: No samples for '{label}'\")\n",
    "        continue\n",
    "    \n",
    "    final_samples, subtypes_info = create_label_file(label, samples)\n",
    "    \n",
    "    # Create filename (replace spaces and special chars)\n",
    "    filename = label.replace(' ', '_').replace(\"'\", \"\") + '.json'\n",
    "    filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "    \n",
    "    # Save\n",
    "    output_data = {\n",
    "        'label': label,\n",
    "        'total_samples': len(final_samples),\n",
    "        'subtypes': {k: len(v) for k, v in subtypes_info.items()},\n",
    "        'samples': final_samples\n",
    "    }\n",
    "    \n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(output_data, f, indent=2)\n",
    "    \n",
    "    print(f\"Created {filename}: {len(final_samples)} samples, {len(subtypes_info)} subtypes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and summarize all created files\n",
    "print(\"\\nFinal Dataset Summary\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "total_samples = 0\n",
    "for filename in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    if filename.endswith('.json') and filename != 'label_subtypes_schema.json':\n",
    "        filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        label = data['label']\n",
    "        count = data['total_samples']\n",
    "        subtypes = data['subtypes']\n",
    "        total_samples += count\n",
    "        \n",
    "        print(f\"{label:30s} {count:5d} samples  ({len(subtypes)} subtypes)\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"{'TOTAL':30s} {total_samples:5d} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Combined Training File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all label files into one training file\n",
    "all_training_samples = []\n",
    "\n",
    "for filename in sorted(os.listdir(OUTPUT_DIR)):\n",
    "    if filename.endswith('.json') and filename != 'label_subtypes_schema.json':\n",
    "        filepath = os.path.join(OUTPUT_DIR, filename)\n",
    "        with open(filepath, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        all_training_samples.extend(data['samples'])\n",
    "\n",
    "# Shuffle\n",
    "random.shuffle(all_training_samples)\n",
    "\n",
    "# Save combined file\n",
    "combined_path = os.path.join(OUTPUT_DIR, '_combined_training.json')\n",
    "with open(combined_path, 'w') as f:\n",
    "    json.dump(all_training_samples, f, indent=2)\n",
    "\n",
    "print(f\"Created combined training file: {len(all_training_samples)} samples\")\n",
    "print(f\"Saved to: {combined_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a few example samples\n",
    "print(\"\\nExample samples:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for i, sample in enumerate(all_training_samples[:3]):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"Text: {sample['text'][:200]}...\")\n",
    "    print(f\"Entities: {sample['entities']}\")\n",
    "    print(f\"Source: {sample['source']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ab_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
