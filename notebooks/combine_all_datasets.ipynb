{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine PII Datasets\n",
    "\n",
    "This notebook combines all unified datasets from the `unified/` folder.\n",
    "\n",
    "**Pre-requisite:** Run `convert_datasets_to_unified_format.py` first to create the unified files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# Paths\n",
    "UNIFIED_DIR = \"/Users/sravan/Documents/Experiments/fintuning_PII/Data/additional_datasets/unified\"\n",
    "OUTPUT_DIR = \"/Users/sravan/Documents/Experiments/fintuning_PII/finetuned-pii-2/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. List Available Unified Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 unified datasets:\n",
      "\n",
      "  beki_privy_unified.json                     54.15 MB\n",
      "  fewnerd_unified.json                        68.83 MB\n",
      "  urchade_unified.json                         2.40 MB\n"
     ]
    }
   ],
   "source": [
    "# List all unified datasets\n",
    "unified_files = [f for f in os.listdir(UNIFIED_DIR) if f.endswith('.json')]\n",
    "print(f\"Found {len(unified_files)} unified datasets:\\n\")\n",
    "\n",
    "for f in sorted(unified_files):\n",
    "    path = os.path.join(UNIFIED_DIR, f)\n",
    "    size_mb = os.path.getsize(path) / (1024 * 1024)\n",
    "    print(f\"  {f:40s} {size_mb:8.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load All Unified Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded beki_privy: 100,951 samples\n",
      "Loaded fewnerd: 131,767 samples\n",
      "Loaded urchade: 19,635 samples\n",
      "\n",
      "Total datasets: 3\n"
     ]
    }
   ],
   "source": [
    "# Load all unified datasets\n",
    "datasets = {}\n",
    "\n",
    "for filename in sorted(unified_files):\n",
    "    path = os.path.join(UNIFIED_DIR, filename)\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    name = filename.replace('_unified.json', '')\n",
    "    datasets[name] = data\n",
    "    print(f\"Loaded {name}: {len(data):,} samples\")\n",
    "\n",
    "print(f\"\\nTotal datasets: {len(datasets)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check Label Distribution per Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "beki_privy\n",
      "============================================================\n",
      "  LOC                              46,328\n",
      "  PER                              28,292\n",
      "  DATE_TIME                        19,516\n",
      "  TITLE                            14,027\n",
      "  NRP                              13,789\n",
      "  FINANCIAL                         8,205\n",
      "  ORG                               5,623\n",
      "  AGE                               2,773\n",
      "  PASSWORD                          2,762\n",
      "  CREDIT_CARD                       2,760\n",
      "  MAC_ADDRESS                       2,748\n",
      "  US_BANK_NUMBER                    2,744\n",
      "  US_DRIVER_LICENSE                 2,737\n",
      "  US_PASSPORT                       2,712\n",
      "  US_LICENSE_PLATE                  2,712\n",
      "  ... and 2 more labels\n",
      "\n",
      "============================================================\n",
      "fewnerd\n",
      "============================================================\n",
      "  LOCATION                         95,339\n",
      "  PERSON                           75,945\n",
      "  ORGANIZATION                     66,920\n",
      "  OTHER                            33,611\n",
      "  PRODUCT                          21,835\n",
      "  BUILDING                         17,599\n",
      "  ART                              14,870\n",
      "  EVENT                            14,061\n",
      "\n",
      "============================================================\n",
      "urchade\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def get_label_counts(data):\n",
    "    \"\"\"Count labels in a dataset\"\"\"\n",
    "    counts = Counter()\n",
    "    for item in data:\n",
    "        for mask in item.get(\"privacy_mask\", []):\n",
    "            counts[mask.get(\"label\", \"UNKNOWN\")] += 1\n",
    "    return counts\n",
    "\n",
    "# Show label distribution for each dataset\n",
    "for name, data in datasets.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"{name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    counts = get_label_counts(data)\n",
    "    for label, count in counts.most_common(15):\n",
    "        print(f\"  {label:30s} {count:>8,}\")\n",
    "    if len(counts) > 15:\n",
    "        print(f\"  ... and {len(counts) - 15} more labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combine All Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all datasets\n",
    "combined = []\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    combined.extend(data)\n",
    "    print(f\"Added {len(data):>10,} from {name}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TOTAL: {len(combined):,} samples\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Overall Label Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall label distribution\n",
    "print(\"Overall Label Distribution:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_labels = get_label_counts(combined)\n",
    "for label, count in all_labels.most_common(30):\n",
    "    print(f\"  {label:30s} {count:>10,}\")\n",
    "\n",
    "print(f\"\\nTotal unique labels: {len(all_labels)}\")\n",
    "print(f\"Total entities: {sum(all_labels.values()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Combined Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save combined dataset\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "output_path = os.path.join(OUTPUT_DIR, \"combined_pii_dataset.json\")\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(combined, f)\n",
    "\n",
    "size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "print(f\"Saved to: {output_path}\")\n",
    "print(f\"File size: {size_mb:.2f} MB\")\n",
    "print(f\"Total samples: {len(combined):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Verify Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample from each source\n",
    "import random\n",
    "\n",
    "sources = set(s.get('source', 'unknown') for s in combined)\n",
    "\n",
    "for source in sorted(sources):\n",
    "    samples = [s for s in combined if s.get('source') == source and s.get('privacy_mask')]\n",
    "    if samples:\n",
    "        sample = random.choice(samples[:100])\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Source: {source}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"Text: {sample['source_text'][:200]}...\")\n",
    "        print(f\"\\nEntities:\")\n",
    "        for mask in sample['privacy_mask'][:5]:\n",
    "            print(f\"  - {mask['label']}: '{mask['value']}' [{mask['start']}:{mask['end']}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ab_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
